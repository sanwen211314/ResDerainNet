name: "Res_DerainNet"
layer {
  name: "data"
  type: "Input"
  top: "rainy"
  #top: "rain_noise"
  input_param {
    shape: { dim: 1 dim: 3 dim: 128 dim: 128 }
  }
  include: { phase: TEST }
}


#1st_layer#########################
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "rainy"
  top: "conv1"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1#0
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
##ReLU_layer
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "relu1"
}
#2nd_layer###########################
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1#0
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

##Batch Normalization
layer {
  name: "conv2-bn"
  type: "BatchNorm"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 1e-8
  }
}
layer {
  name: "conv2-bn-scale"
  type: "Scale"
  bottom: "conv2"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    axis: 1
    num_axes: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}  
##ReLU_layer
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "relu2"
}
#3rd_layer###########################
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "relu2"
  top: "conv3"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

##Batch Normalization
layer {
  name: "conv3-bn"
  type: "BatchNorm"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 1e-8
  }
}
layer {
  name: "conv3-bn-scale"
  type: "Scale"
  bottom: "conv3"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    axis: 1
    num_axes: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}  
##ReLU_layer
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "relu3"
}
#4th_layer###########################
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

##Batch Normalization
layer {
  name: "conv4-bn"
  type: "BatchNorm"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 1e-8
  }
}
layer {
  name: "conv4-bn-scale"
  type: "Scale"
  bottom: "conv4"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    axis: 1
    num_axes: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}  
##ReLU_layer
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "relu4"
}

#5th_layer###########################
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "relu4"
  top: "conv5"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

##Batch Normalization
layer {
  name: "conv5-bn"
  type: "BatchNorm"
  bottom: "conv5"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 1e-8
  }
}
layer {
  name: "conv5-bn-scale"
  type: "Scale"
  bottom: "conv5"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    axis: 1
    num_axes: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}  
##ReLU_layer
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "relu5"
}
#6th_layer###########################
layer {
  name: "conv6"
  type: "Convolution"
  bottom: "relu5"
  top: "conv6"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

##Batch Normalization
layer {
  name: "conv6-bn"
  type: "BatchNorm"
  bottom: "conv6"
  top: "conv6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 1e-8
  }
}
layer {
  name: "conv6-bn-scale"
  type: "Scale"
  bottom: "conv6"
  top: "conv6"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    axis: 1
    num_axes: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}  
##ReLU_layer
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "conv6"
  top: "relu6"
}
#7th_layer###########################
layer {
  name: "conv7"
  type: "Convolution"
  bottom: "relu6"
  top: "conv7"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

##Batch Normalization
layer {
  name: "conv7-bn"
  type: "BatchNorm"
  bottom: "conv7"
  top: "conv7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 1e-8
  }
}
layer {
  name: "conv7-bn-scale"
  type: "Scale"
  bottom: "conv7"
  top: "conv7"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    axis: 1
    num_axes: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}  
##ReLU_layer
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "conv7"
  top: "relu7"
}
#8th_layer###########################
layer {
  name: "conv8"
  type: "Convolution"
  bottom: "relu7"
  top: "conv8"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

##Batch Normalization
layer {
  name: "conv8-bn"
  type: "BatchNorm"
  bottom: "conv8"
  top: "conv8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 1e-8
  }
}
layer {
  name: "conv8-bn-scale"
  type: "Scale"
  bottom: "conv8"
  top: "conv8"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    axis: 1
    num_axes: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}  
##ReLU_layer
layer {
  name: "relu8"
  type: "ReLU"
  bottom: "conv8"
  top: "relu8"
}
#9th_layer###########################
layer {
  name: "conv9"
  type: "Convolution"
  bottom: "relu8"
  top: "conv9"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

##Batch Normalization
layer {
  name: "conv9-bn"
  type: "BatchNorm"
  bottom: "conv9"
  top: "conv9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 1e-8
  }
}
layer {
  name: "conv9-bn-scale"
  type: "Scale"
  bottom: "conv9"
  top: "conv9"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    axis: 1
    num_axes: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}  
##ReLU_layer
layer {
  name: "relu9"
  type: "ReLU"
  bottom: "conv9"
  top: "relu9"
}

#10th_layer###########################
layer {
  name: "conv10"
  type: "Convolution"
  bottom: "relu9"
  top: "conv10"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

##Batch Normalization
layer {
  name: "conv10-bn"
  type: "BatchNorm"
  bottom: "conv10"
  top: "conv10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 1e-8
  }
}
layer {
  name: "conv10-bn-scale"
  type: "Scale"
  bottom: "conv10"
  top: "conv10"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    axis: 1
    num_axes: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}  
##ReLU_layer
layer {
  name: "relu10"
  type: "ReLU"
  bottom: "conv10"
  top: "relu10"
}

#11th_layer###########################
layer {
  name: "conv11"
  type: "Convolution"
  bottom: "relu10"
  top: "conv11"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

##Batch Normalization
layer {
  name: "conv11-bn"
  type: "BatchNorm"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 1e-8
  }
}
layer {
  name: "conv11-bn-scale"
  type: "Scale"
  bottom: "conv11"
  top: "conv11"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    axis: 1
    num_axes: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}  
##ReLU_layer
layer {
  name: "relu11"
  type: "ReLU"
  bottom: "conv11"
  top: "relu11"
}
#12nd_layer###########################
layer {
  name: "conv12"
  type: "Convolution"
  bottom: "relu11"
  top: "conv12"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

##Batch Normalization
layer {
  name: "conv12-bn"
  type: "BatchNorm"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 1e-8
  }
}
layer {
  name: "conv12-bn-scale"
  type: "Scale"
  bottom: "conv12"
  top: "conv12"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    axis: 1
    num_axes: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}  
##ReLU_layer
layer {
  name: "relu12"
  type: "ReLU"
  bottom: "conv12"
  top: "relu12"
}
#13rd_layer###########################
layer {
  name: "conv13"
  type: "Convolution"
  bottom: "relu12"
  top: "conv13"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

##Batch Normalization
layer {
  name: "conv13-bn"
  type: "BatchNorm"
  bottom: "conv13"
  top: "conv13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 1e-8
  }
}
layer {
  name: "conv13-bn-scale"
  type: "Scale"
  bottom: "conv13"
  top: "conv13"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    axis: 1
    num_axes: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}  
##ReLU_layer
layer {
  name: "relu13"
  type: "ReLU"
  bottom: "conv13"
  top: "relu13"
}
#14th_layer###########################
layer {
  name: "conv14"
  type: "Convolution"
  bottom: "relu13"
  top: "conv14"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

##Batch Normalization
layer {
  name: "conv14-bn"
  type: "BatchNorm"
  bottom: "conv14"
  top: "conv14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 1e-8
  }
}
layer {
  name: "conv14-bn-scale"
  type: "Scale"
  bottom: "conv14"
  top: "conv14"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    axis: 1
    num_axes: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}  
##ReLU_layer
layer {
  name: "relu14"
  type: "ReLU"
  bottom: "conv14"
  top: "relu14"
}

#15th_layer###########################
layer {
  name: "conv15"
  type: "Convolution"
  bottom: "relu14"
  top: "conv15"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

##Batch Normalization
layer {
  name: "conv15-bn"
  type: "BatchNorm"
  bottom: "conv15"
  top: "conv15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 1e-8
  }
}
layer {
  name: "conv15-bn-scale"
  type: "Scale"
  bottom: "conv15"
  top: "conv15"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    axis: 1
    num_axes: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}  
##ReLU_layer
layer {
  name: "relu15"
  type: "ReLU"
  bottom: "conv15"
  top: "relu15"
}
#16th_layer###########################
layer {
  name: "conv16"
  type: "Convolution"
  bottom: "relu15"
  top: "conv16"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

##Batch Normalization
layer {
  name: "conv16-bn"
  type: "BatchNorm"
  bottom: "conv16"
  top: "conv16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 1e-8
  }
}
layer {
  name: "conv16-bn-scale"
  type: "Scale"
  bottom: "conv16"
  top: "conv16"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    axis: 1
    num_axes: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}  
##ReLU_layer
layer {
  name: "relu16"
  type: "ReLU"
  bottom: "conv16"
  top: "relu16"
}
#17th_layer###########################
layer {
  name: "conv17"
  type: "Convolution"
  bottom: "relu16"
  top: "conv17"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

##Batch Normalization
layer {
  name: "conv17-bn"
  type: "BatchNorm"
  bottom: "conv17"
  top: "conv17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 1e-8
  }
}
layer {
  name: "conv17-bn-scale"
  type: "Scale"
  bottom: "conv17"
  top: "conv17"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    axis: 1
    num_axes: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}  
##ReLU_layer
layer {
  name: "relu17"
  type: "ReLU"
  bottom: "conv17"
  top: "relu17"
}
#18th_layer###########################
layer {
  name: "conv18"
  type: "Convolution"
  bottom: "relu17"
  top: "conv18"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

##Batch Normalization
layer {
  name: "conv18-bn"
  type: "BatchNorm"
  bottom: "conv18"
  top: "conv18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 1e-8
  }
}
layer {
  name: "conv18-bn-scale"
  type: "Scale"
  bottom: "conv18"
  top: "conv18"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    axis: 1
    num_axes: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}  
##ReLU_layer
layer {
  name: "relu18"
  type: "ReLU"
  bottom: "conv18"
  top: "relu18"
}
#19th_layer###########################
layer {
  name: "conv19"
  type: "Convolution"
  bottom: "relu18"
  top: "conv19"
  convolution_param {
    num_output: 64
    kernel_size: 3
    stride: 1
    pad: 1
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

##Batch Normalization
layer {
  name: "conv19-bn"
  type: "BatchNorm"
  bottom: "conv19"
  top: "conv19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 1e-8
  }
}
layer {
  name: "conv19-bn-scale"
  type: "Scale"
  bottom: "conv19"
  top: "conv19"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 1
    decay_mult: 1
  }
  scale_param {
    axis: 1
    num_axes: 1
    filler {
      type: "constant"
      value: 1
    }
    bias_term: true
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}  
##ReLU_layer
layer {
  name: "relu19"
  type: "ReLU"
  bottom: "conv19"
  top: "relu19"
}

#last_layer############################
layer {
  name: "conv20"
  type: "Convolution"
  bottom: "relu19"
  top: "conv20"
  convolution_param {
    num_output: 3
    kernel_size: 3
    stride: 1
    pad: 1#0
    weight_filler {
      type: "msra"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
